Presentation attacks are a prevalent security concern today. Imposters 
attempt to gain access to restricted resources using fake biometric data such as face, fngerprint, or iris
images. Various presentation attack detection (PAD) systems have
been designed leveraging deep learning (DL) models. But in most
cases, DL models function as black boxes - their decisions are
opaque to their users. The purpose of explainability techniques
is to provide detailed information about the reason behind the
behavior/decision of DL models. Visual explanation is necessary
to better understand the decisions/predictions of DL-based PAD
systems and determine the key regions due to which a biometric
image is considered real or fake by the system. In this work,
a novel technique Ensemble-CAM is proposed for providing
visual explanations for the decisions made by deep learning-based
face PAD systems. Our goal is to improve DL-based face PAD
systems by providing a better understanding of their behavior.
Our provided visual explanations will enhance the transparency
and trustworthiness of DL-based face PAD systems.



If you need the trained model and dataset, you can email shadmar@clarkson.edu. 
